{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, model_selection\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "#load data\n",
    "def get_data(path, n_features=None):\n",
    "    if n_features == None:\n",
    "        X, y = datasets.load_svmlight_file(path)\n",
    "    else:\n",
    "        X, y = datasets.load_svmlight_file(path, n_features=n_features)\n",
    "    #append one column\n",
    "    X = np.hstack([X.toarray(), np.ones((X.shape[0], 1))])\n",
    "    y = np.array(y).reshape(X.shape[0],1)\n",
    "    y[y==-1] = 0 #if y == -1, then y = 0\n",
    "    return X, y\n",
    "\n",
    "#loss_function\n",
    "#def compute_loss(X, Y, theta, C=1):\n",
    " #   epsilon_loss = 1 - Y * X.dot(theta)\n",
    "  #  epsilon_loss[epsilon_loss<0] = 0\n",
    "   # loss = 0.5 * np.dot(theta.transpose(), theta).sum() + C*epsilon_loss.sum()\n",
    "    #return loss/X.shape[0]\n",
    "def compute_loss(X, y, theta, C):\n",
    "    loss = (np.linalg.norm(theta,2)**2)/2 + C * np.mean(np.maximum(1 - y * X.dot(theta.T), 0))\n",
    "    return loss\n",
    "    \n",
    "#gradient value\n",
    "def gradient(X_train, Y_train, theta, C):\n",
    "    grad = C*grad/X_train.shape[0] + theta\n",
    "    return grad\n",
    "\n",
    "#get part of sample\n",
    "def get_part(X, y ,min_part):\n",
    "    i = np.random.randint(0, X.shape[0], size=min_part, dtype=int) #generate random int from 0 to 32561\n",
    "    return X[i,:], y[i]    \n",
    "\n",
    "#show function\n",
    "def show(train_loss, test_loss):\n",
    "    plt.plot(train_loss,'red',label='Train')\n",
    "    plt.plot(test_loss,'black',label='test')\n",
    "    plt.xlabel('round number')\n",
    "    plt.ylabel('Loss value')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticClassification(X_train, y_train, theta, C,\n",
    "                           learning_rate,\n",
    "                           optimizer=None, \n",
    "                           optimizer_params=None):\n",
    "    if optimizer == None:\n",
    "        gred = gradient(X_train, y_train, theta)\n",
    "        theta = theta - learning_rate*gred\n",
    "    elif optimizer == \"NAG\":\n",
    "        #initialize v and Gamma\n",
    "        v = np.zeros(theta.shape)\n",
    "        Gamma = 0.9\n",
    "            \n",
    "        grad = gradient(X_train, y_train, theta- Gamma*v)\n",
    "        v = Gamma*v + learning_rate*grad\n",
    "        theta = theta - v\n",
    "\n",
    "        #optimizer_params['Velocity'] = v\n",
    "    elif optimizer == \"RMSProp\":\n",
    "        G = np.zeros(theta.shape)\n",
    "        Gamma = 0.9\n",
    "        Epsilon = 1e-7\n",
    "\n",
    "        grad = gradient(X_train, y_train, theta)\n",
    "        G = Gamma*G + (1-Gamma)*(grad**2)            \n",
    "        theta = theta - learning_rate*grad/(np.sqrt(G+Epsilon))\n",
    "            \n",
    "        #optimizer_params['Velocity'] = G\n",
    "    elif optimizer == \"Adadelta\":\n",
    "        G = np.zeros(theta.shape)\n",
    "        Delta = np.zeros(theta.shape)\n",
    "        Gamma = 0.9\n",
    "        Epsilon = 1e-7\n",
    "\n",
    "        grad = gradient(X_train, y_train, theta)\n",
    "        G = Gamma*G + (1-Gamma)*(grad**2)\n",
    "        DeltaTheta = -((np.sqrt(Delta+Epsilon))/(np.sqrt(G+Epsilon)))*grad\n",
    "        \n",
    "        theta = theta + DeltaTheta\n",
    "        Delta = Delta*Gamma + (1-Gamma)*(DeltaTheta**2)\n",
    "            \n",
    "        #optimizer_params['Delta'] = Delta\n",
    "        #optimizer_params['G'] = G\n",
    "    elif optimizer == \"Adam\":\n",
    "        m = np.zeros(theta.shape)\n",
    "        G = np.zeros(theta.shape)\n",
    "        Alpha = np.zeros(theta.shape)\n",
    "        Beta = 0.9\n",
    "        Gamma = 0.999\n",
    "        Epsilon = 1e-8\n",
    "            \n",
    "        grad = gradient(X_train, y_train, theta)\n",
    "        m = Beta*m + (1-Beta)*grad\n",
    "        G = Gamma*G + (1-Gamma)*(grad**2)\n",
    "        Alpha = learning_rate * (np.sqrt(1-Gamma))/(1-Beta)\n",
    "        theta = theta - Alpha*m/(np.sqrt(G + Epsilon))\n",
    "\n",
    "        #optimizer_params['m'] = m\n",
    "        #optimizer_params['G'] = G\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main(learning_rate=0.0003,\n",
    "         C=5,\n",
    "         threshold=0,\n",
    "         min_part=64, \n",
    "         num_round=2000,     \n",
    "         optimizer=None):\n",
    "    trian_path = 'C:/Users/47864/Desktop/Data/a9a.txt'\n",
    "    test_path = 'C:/Users/47864/Desktop/Data/a9a.t'\n",
    "    X_train, y_train = get_data(trian_path)\n",
    "    X_test, y_test = get_data(test_path, X_train.shape[1]-1)\n",
    "    theta = np.random.random((X_train.shape[1], 1)) #random initialize\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    for item in range(num_round):\n",
    "        X_train_part, y_train_part = get_part(X_train, y_train, min_part)\n",
    "        theta = LogisticClassification(X_train_part, y_train_part, theta, learning_rate=learning_rate, optimizer=optimizer)\n",
    "        \n",
    "        loss = compute_loss(X_train_part, y_train_part, theta)\n",
    "        train_loss_history.append(loss)\n",
    "        \n",
    "        X_test_part, y_test_part = get_part(X_test, y_test, min_part)\n",
    "        loss = compute_loss(X_test_part, y_test_part, theta)\n",
    "        test_loss_history.append(loss)\n",
    "    \n",
    "    \n",
    "    if optimizer != None:\n",
    "        print(optimizer)\n",
    "    if optimizer == None:\n",
    "        print(\"without optimizer\")\n",
    "    show(train_loss_history, test_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LogisticClassification() missing 1 required positional argument: 'C'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-64e57f4f218b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m      \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m      \u001b[0mmin_part\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m      num_round=2000)\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#NAG\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m main(learning_rate=0.003,\n",
      "\u001b[1;32m<ipython-input-12-52639d20c332>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(learning_rate, C, threshold, min_part, num_round, optimizer)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_round\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mX_train_part\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_part\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_part\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_part\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticClassification\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_part\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_part\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_part\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_part\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: LogisticClassification() missing 1 required positional argument: 'C'"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "main(learning_rate=0.003,\n",
    "     C=5,\n",
    "     threshold=0,\n",
    "     min_part=128,\n",
    "     num_round=2000)\n",
    "#NAG\n",
    "main(learning_rate=0.003,\n",
    "     C=5,\n",
    "     threshold=0,\n",
    "     min_part=128,\n",
    "     num_round=2000,\n",
    "     optimizer='NAG')\n",
    "#RMSProp\n",
    "main(learning_rate=0.003,\n",
    "     C=5,\n",
    "     threshold=0.5,\n",
    "     min_part=1024,\n",
    "     num_round=2000,\n",
    "     optimizer='RMSProp')\n",
    "#Adadelta\n",
    "main(learning_rate=0.001,\n",
    "     C=5,\n",
    "     threshold=0.5,\n",
    "     min_part=1024,\n",
    "     num_round=2000,\n",
    "     optimizer='Adadelta')\n",
    "#Adam\n",
    "main(learning_rate=0.001,\n",
    "     C=5,\n",
    "     threshold=0.5,\n",
    "     min_part=1024,\n",
    "     num_round=2000,\n",
    "     optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
